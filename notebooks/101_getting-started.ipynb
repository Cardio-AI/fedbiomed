{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed-BioMed Researcher base example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the node up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 2 (default) to add MNIST to the node\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MNIST is downloaded (this is due to a pytorch issue https://github.com/pytorch/vision/issues/3549)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node start`. Wait until you get `Starting task manager`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an experiment model and parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch training plan MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    # Defines and return model \n",
    "    def init_model(self, model_args):\n",
    "        return self.Net(model_args = model_args)\n",
    "    \n",
    "    # Defines and return optimizer\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n",
    "    \n",
    "    # Declares and return dependencies\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\"]\n",
    "        return deps\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(9216, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "\n",
    "\n",
    "            output = F.log_softmax(x, dim=1)\n",
    "            return output\n",
    "\n",
    "    def training_data(self):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = { 'shuffle': True}\n",
    "        return DataManager(dataset=dataset1, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 48, }, \n",
    "    'optimizer_args': {\n",
    "        \"lr\" : 1e-3\n",
    "    },\n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 14:50:46,198 fedbiomed INFO - Searching dataset with data tags: ['#MNIST', '#dataset'] for all nodes\n",
      "2023-11-22 14:50:46,205 fedbiomed DEBUG - Node: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d polling for the tasks\n",
      "2023-11-22 14:50:56,211 fedbiomed INFO - Node selected for training -> node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d\n",
      "2023-11-22 14:50:56,219 fedbiomed DEBUG - Model file has been saved: /home/scansiz/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0010/my_model_2e802371-556b-4378-a0f4-de1fde33d7e5.py\n",
      "2023-11-22 14:50:56,259 fedbiomed DEBUG - using native torch optimizer\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environ[\"DB_PATH\"]\n",
    "environ[\"MESSAGES_QUEUE_DIR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 14:50:58,804 fedbiomed INFO - Sampled nodes in round 0 ['node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d']\n",
      "2023-11-22 14:50:58,817 fedbiomed INFO - \u001b[1mSending request\u001b[0m \n",
      "\t\t\t\t\t\u001b[1m To\u001b[0m: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t\u001b[1m Request: \u001b[0m: TRAIN\n",
      " -----------------------------------------------------------------\n",
      "2023-11-22 14:50:58,915 fedbiomed DEBUG - Node: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d polling for the tasks\n",
      "2023-11-22 14:50:59,104 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 1/100 (1%) | Samples: 48/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m2.289945\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:50:59,563 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 10/100 (10%) | Samples: 480/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m1.186747\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:00,114 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 20/100 (20%) | Samples: 960/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m1.033792\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:00,635 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 30/100 (30%) | Samples: 1440/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.823118\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:01,237 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 40/100 (40%) | Samples: 1920/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.265791\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:01,778 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 50/100 (50%) | Samples: 2400/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.705375\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:02,318 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 60/100 (60%) | Samples: 2880/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.477794\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:02,872 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 70/100 (70%) | Samples: 3360/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.404316\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:03,426 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 80/100 (80%) | Samples: 3840/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.430526\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:04,009 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 90/100 (90%) | Samples: 4320/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.380325\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:04,781 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 100/100 (100%) | Samples: 4800/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.340161\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:13,946 fedbiomed INFO - Nodes that successfully reply in round 0 ['node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d']\n",
      "2023-11-22 14:51:13,966 fedbiomed INFO - Saved aggregated params for round 0 in /home/scansiz/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0010/aggregated_params_01215144-9bc8-43d6-950a-d6cbdd7f8e77.mpk\n",
      "2023-11-22 14:51:13,966 fedbiomed INFO - Sampled nodes in round 1 ['node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d']\n",
      "2023-11-22 14:51:13,970 fedbiomed INFO - \u001b[1mSending request\u001b[0m \n",
      "\t\t\t\t\t\u001b[1m To\u001b[0m: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t\u001b[1m Request: \u001b[0m: TRAIN\n",
      " -----------------------------------------------------------------\n",
      "2023-11-22 14:51:14,002 fedbiomed DEBUG - Node: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d polling for the tasks\n",
      "2023-11-22 14:51:14,158 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 1/100 (1%) | Samples: 48/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.247096\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:14,627 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 10/100 (10%) | Samples: 480/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.349592\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:15,136 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 20/100 (20%) | Samples: 960/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.189548\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:15,661 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 30/100 (30%) | Samples: 1440/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.270794\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:16,166 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 40/100 (40%) | Samples: 1920/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.177289\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:16,703 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 50/100 (50%) | Samples: 2400/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.394667\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:17,208 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 60/100 (60%) | Samples: 2880/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.192374\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:17,725 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 70/100 (70%) | Samples: 3360/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.269862\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:18,228 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 80/100 (80%) | Samples: 3840/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.082721\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:18,771 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 90/100 (90%) | Samples: 4320/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.087100\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:19,270 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 100/100 (100%) | Samples: 4800/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.296649\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-11-22 14:51:29,093 fedbiomed INFO - Nodes that successfully reply in round 1 ['node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d']\n",
      "2023-11-22 14:51:29,108 fedbiomed INFO - Saved aggregated params for round 1 in /home/scansiz/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0010/aggregated_params_f8826a1b-5036-4a53-953b-b346d82d4b1b.mpk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 14:52:09,597 fedbiomed WARNING - Node node_64aeb5ba-8a0d-49ef-af4b-986b5e564e5d is disconnected. Request/task that are created for this node will be flushed\n"
     ]
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feel free to run other sample notebooks or try your own models :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
