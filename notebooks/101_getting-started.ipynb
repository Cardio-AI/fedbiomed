{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed-BioMed Researcher base example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the node up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 2 (default) to add MNIST to the node\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MNIST is downloaded (this is due to a pytorch issue https://github.com/pytorch/vision/issues/3549)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node start`. Wait until you get `Starting task manager`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an experiment model and parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch training plan MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": null,
>>>>>>> d4bdf88d852bdeb65a736e76d79d1c1018f70716
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    # Defines and return model \n",
    "    def init_model(self, model_args):\n",
    "        return self.Net(model_args = model_args)\n",
    "    \n",
    "    # Defines and return optimizer\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n",
    "    \n",
    "    # Declares and return dependencies\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\"]\n",
    "        return deps\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(9216, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "\n",
    "\n",
    "            output = F.log_softmax(x, dim=1)\n",
    "            return output\n",
    "\n",
    "    def training_data(self):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = { 'shuffle': True}\n",
    "        return DataManager(dataset=dataset1, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> d4bdf88d852bdeb65a736e76d79d1c1018f70716
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 48, }, \n",
    "    'optimizer_args': {\n",
    "        \"lr\" : 1e-3\n",
    "    },\n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> d4bdf88d852bdeb65a736e76d79d1c1018f70716
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 17:19:29,578 fedbiomed INFO - Starting researcher service...\n",
      "2023-09-13 17:19:31,578 fedbiomed INFO - Received request form node_41533df5-d07b-4027-a826-d1f67410d627\n",
      "2023-09-13 17:19:32,597 fedbiomed INFO - Searching dataset with data tags: ['#MNIST', '#dataset'] for all nodes\n",
      "2023-09-13 17:19:32,614 fedbiomed INFO - Received request form node_41533df5-d07b-4027-a826-d1f67410d627\n",
      "2023-09-13 17:19:42,612 fedbiomed INFO - Node selected for training -> node_41533df5-d07b-4027-a826-d1f67410d627\n",
      "Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> d4bdf88d852bdeb65a736e76d79d1c1018f70716
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 17:19:44,107 fedbiomed INFO - Sampled nodes in round 0 ['node_41533df5-d07b-4027-a826-d1f67410d627']\n",
      "2023-09-13 17:19:44,113 fedbiomed INFO - \u001b[1mSending request\u001b[0m \n",
      "\t\t\t\t\t\u001b[1m To\u001b[0m: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t\u001b[1m Request: \u001b[0m: TRAIN\n",
      " -----------------------------------------------------------------\n",
      "2023-09-13 17:19:44,246 fedbiomed INFO - Received request form node_41533df5-d07b-4027-a826-d1f67410d627\n",
      "2023-09-13 17:19:44,652 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 1/100 (1%) | Samples: 48/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m2.272448\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:45,342 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 10/100 (10%) | Samples: 480/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m1.315238\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:46,229 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 20/100 (20%) | Samples: 960/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.907969\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:47,154 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 30/100 (30%) | Samples: 1440/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.578620\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:47,892 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 40/100 (40%) | Samples: 1920/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.497070\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:48,618 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 50/100 (50%) | Samples: 2400/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.372798\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:49,206 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 60/100 (60%) | Samples: 2880/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.453164\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:50,334 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 70/100 (70%) | Samples: 3360/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.307941\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:51,012 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 80/100 (80%) | Samples: 3840/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.547767\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:51,650 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 90/100 (90%) | Samples: 4320/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.381311\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:52,295 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 100/100 (100%) | Samples: 4800/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.233451\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:19:59,162 fedbiomed INFO - Nodes that successfully reply in round 0 ['node_41533df5-d07b-4027-a826-d1f67410d627']\n",
      "2023-09-13 17:19:59,197 fedbiomed INFO - Saved aggregated params for round 0 in /user/scansiz/home/projects/fedbiomed-dev/fedbiomed/var/experiments/Experiment_0169/aggregated_params_f46142b1-ff93-4074-ac74-7197b6ba891a.mpk\n",
      "2023-09-13 17:19:59,198 fedbiomed INFO - Sampled nodes in round 1 ['node_41533df5-d07b-4027-a826-d1f67410d627']\n",
      "2023-09-13 17:19:59,200 fedbiomed INFO - \u001b[1mSending request\u001b[0m \n",
      "\t\t\t\t\t\u001b[1m To\u001b[0m: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t\u001b[1m Request: \u001b[0m: TRAIN\n",
      " -----------------------------------------------------------------\n",
      "2023-09-13 17:19:59,248 fedbiomed INFO - Received request form node_41533df5-d07b-4027-a826-d1f67410d627\n",
      "2023-09-13 17:19:59,646 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 1/100 (1%) | Samples: 48/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.137893\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:00,579 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 10/100 (10%) | Samples: 480/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.273856\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:01,503 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 20/100 (20%) | Samples: 960/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.373933\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:02,319 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 30/100 (30%) | Samples: 1440/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.375131\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:02,964 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 40/100 (40%) | Samples: 1920/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.084991\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:03,969 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 50/100 (50%) | Samples: 2400/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.136581\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:05,008 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 60/100 (60%) | Samples: 2880/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.262040\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:05,758 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 70/100 (70%) | Samples: 3360/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.166384\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:06,961 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 80/100 (80%) | Samples: 3840/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.335103\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:08,134 fedbiomed INFO - \u001b[1mTRAINING\u001b[0m \n",
      "\t\t\t\t\t NODE_ID: node_41533df5-d07b-4027-a826-d1f67410d627 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 90/100 (90%) | Samples: 4320/4800\n",
      " \t\t\t\t\t Loss: \u001b[1m0.307812\u001b[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-13 17:20:19,273 fedbiomed WARNING - Node node_41533df5-d07b-4027-a826-d1f67410d627 is disconnected. Request/task that are created for this node will be flushed\n",
      "2023-09-13 17:20:38,019 fedbiomed CRITICAL - Fed-BioMed researcher stopped due to keyboard interrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "Fed-BioMed researcher stopped due to keyboard interrupt\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "FedbiomedSilentTerminationError",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feel free to run other sample notebooks or try your own models :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
