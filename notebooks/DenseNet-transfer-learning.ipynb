{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033d2892-772f-4cf5-ba04-d2aefb9fb169",
   "metadata": {},
   "source": [
    "# Brain tumors classification using DenseNet Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c1366d-b1b5-4137-8ac5-4212211c5754",
   "metadata": {},
   "source": [
    "This tutorial presents an example of Transfer Learning with DenseNet model to classify images in tumor or healthy brain images.\n",
    "It is interessant to perfom transfer-learning when you have not enough data to train a model.\n",
    "\n",
    "You could use pretrained model with Fed-BioMed and classify your data and/or try to fine-tune the last layers of a pre-trained model with your specific data. This startegy could lead to better classifications results in you have enough data. \n",
    "\n",
    "Here is an example With a public dataset of MRI (Brain dataset from Kaggle).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811849b1-5047-48d4-b745-93efcb33220d",
   "metadata": {},
   "source": [
    "## About the model\n",
    "\n",
    "The model used is Densenet-121 model(“Densely Connected Convolutional Networks”) pretrained on ImageNet dataset. The Pytorch pretrained Densenet121 is used https://pytorch.org/vision/main/generated/torchvision.models.densenet121.html to perform image classification on the MedNIST dataset. The goal of this Densenet121 model is to predict the class of the image modality given the medical image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc88874-e1eb-4aa5-bb48-f8519b0b0753",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a22a823-452d-454c-a856-5828cf429aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image # A module from the Python Imaging Library (PIL) that provides functionality for opening, manipulating, and saving various image file formats.\n",
    "import pathlib\n",
    "import os # A module that provides a way to interact with the operating system, allowing for tasks such as file and directory manipulation.\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms as transforms # A module from the torchvision library that provides common image transformations, such as resizing, cropping, and normalization.\n",
    "import torch.nn as nn # A module in PyTorch that provides classes for defining and building neural networks.\n",
    "from torchvision import utils # A module from torchvision that contains utility functions for working with images, such as saving and visualizing them.\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.optim as optim\n",
    "import itertools # This import statement imports the itertools module, which provides functions for efficient looping and combining of iterables. It can be used for tasks such as generating combinations or permutations of elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac384313-ff90-4192-969b-53a42ccad934",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "Upload on a node the metadata.csv containing images and labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31caee0b-c716-4a4d-95b4-426ae4b01fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | image           | class   | format   | mode   | shape         |\n",
      "|---:|:----------------|:--------|:---------|:-------|:--------------|\n",
      "|  0 | Cancer (1).jpg  | tumor   | JPEG     | RGB    | (512, 512, 3) |\n",
      "|  1 | Cancer (1).png  | tumor   | PNG      | L      | (300, 240)    |\n",
      "|  2 | Cancer (1).tif  | tumor   | TIFF     | RGB    | (256, 256, 3) |\n",
      "|  3 | Cancer (10).jpg | tumor   | JPEG     | RGB    | (512, 512, 3) |\n",
      "|  4 | Cancer (10).tif | tumor   | TIFF     | RGB    | (256, 256, 3) |\n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_csv(\"/user/ebirgy/home/Downloads/Brain_Tumor_IRM/metadata.csv\") # paths needs to be completed \n",
    "labels_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "print(labels_df.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f2d2ba-6384-4e9d-b977-9f85208a2a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_df[\"class\"].value_counts().plot(kind='bar')\n",
    "plt.title('Labels counts')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a123c5-ef5a-40e4-a2eb-4e6eb7082d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding des categories : création d'un dictionnaire\n",
    "label={'tumor': \"1\", 'normal': \"0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec49ce-d836-4de6-9f2f-571ed72362a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label=dict(label)\n",
    "labels_df[\"label\"]=labels_df['class'].map(dict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc504559-c1a2-43a3-bba3-14dbbdcc3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where the images are located\n",
    "path = \"/user/ebirgy/home/Downloads/Brain_Tumor_IRM/\"\n",
    "# Add a new column 'full_path' to the DataFrame\n",
    "labels_df['path_images'] = path + labels_df['image']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feec6df-cafe-4ccc-838f-3bf92b4c9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb74cc7-df71-41fc-a260-180909ca722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=labels_df[['path_images', 'label', 'image']]\n",
    "dataset['label']=dataset['label'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8c880-060b-4c1f-9e45-89197814215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26316ca-adb3-493a-8904-faac235cfb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"../cancer_labels.csv\")\n",
    "df = pd.read_csv(\"../cancer_labels.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef641434-e45f-419d-8875-44b85d0c86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41ea09-48d4-4408-ad50-5f3670a94771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req  = Requests()\n",
    "req.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4b254-fed4-4c55-92ce-695127c7db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import TorchTrainingPlan \n",
    "from fedbiomed.common.data import DataManager\n",
    "from torch.utils.data import Dataset\n",
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b55b89-75f7-4d6e-8b2e-efcccbcbcd1c",
   "metadata": {},
   "source": [
    "## Use pre-trained model to classify your images data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef11e8-0908-44cd-a61b-c7a065ffcfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainingPlan2(TorchTrainingPlan):\n",
    "\n",
    "    def init_model(self, model_args):\n",
    "        # Define your custom classification layer\n",
    "        class CustomClassifier(nn.Module):\n",
    "            def __init__(self, in_features, num_classes):\n",
    "                super(CustomClassifier, self).__init__()\n",
    "                self.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "            def forward(self, x):\n",
    "                x = self.fc(x)\n",
    "                return x\n",
    "        \n",
    "        # Load the pre-trained DenseNet model\n",
    "        model = models.densenet121(pretrained=True)\n",
    "        \n",
    "        # Modify the classifier layer\n",
    "        in_features = model.classifier.in_features\n",
    "        num_classes = model_args['num_classes']  # Change this to the number of classes in your dataset\n",
    "        custom_classifier = CustomClassifier(in_features, num_classes)\n",
    "        \n",
    "        # Replace the classifier\n",
    "        model.classifier = custom_classifier\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def init_dependencies(self):\n",
    "        return [\n",
    "            \"import pandas as pd\",\n",
    "            \"from PIL import Image\",\n",
    "            \"from torchvision import datasets, transforms, models\",\n",
    "            \"from torch.utils.data import Dataset\",\n",
    "            \"import torch.optim as optim\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    def init_optimizer(self, optimizer_args):        \n",
    "        return optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])\n",
    "\n",
    "    # training data\n",
    "    class CustomDatasetFedBioMed(Dataset):\n",
    "        def __init__(self, dataset_path: str, transform=None):\n",
    "            self.dataframe = pd.read_csv(dataset_path, index_col=0)\n",
    "            \n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),  \n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(30),\n",
    "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "           ])\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.dataframe)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            \n",
    "            img_path = self.dataframe.iloc[idx, 0]  # the file path is in the first column\n",
    "            print(img_path)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "            label = self.dataframe.iloc[idx, 1]  # the class label is in the second column\n",
    "    \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "    \n",
    "            label = torch.tensor(label, dtype=torch.long)  # if it returns a tuple instead of a tensor\n",
    "    \n",
    "            return image, label \n",
    "\n",
    "    def training_data(self):\n",
    "        dataset = self.CustomDatasetFedBioMed(self.dataset_path)\n",
    "        return DataManager(dataset)\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        loss   = loss_func(output, target)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029bcdd-19a8-400c-a9ba-b8f0de1b00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 32, }, \n",
    "    'optimizer_args': {'lr': 1e-4}, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'num_classes': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4bf44-3f0d-4a28-8a2e-52fd0a921caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tags =  ['cancer']\n",
    "rounds = 1\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 training_plan_class=MyTrainingPlan2,\n",
    "                 model_args=model_args,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage())\n",
    "\n",
    "from fedbiomed.common.metrics import MetricTypes\n",
    "exp.set_test_ratio(.1)\n",
    "exp.set_test_on_local_updates(True)\n",
    "exp.set_test_metric(MetricTypes.ACCURACY)\n",
    "\n",
    "exp.set_tensorboard(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9d05a-98ed-4d3d-affd-3da9e2ecbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7236f29-576a-4c94-ae9c-54695f0c8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./training_plan2_densenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d0941-fcb4-485a-a3b8-c2a0107956f9",
   "metadata": {},
   "source": [
    "## Fine-tune : train last layers before classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26cf87a-268a-46dd-96a4-9e5f83e0ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainingPlan3(TorchTrainingPlan):\n",
    "\n",
    "    def init_model(self, model_args):\n",
    "        model = models.densenet121(pretrained=True)\n",
    "        # For example, let's freeze all layers up to the second-to-last dense block\n",
    "        for param in model.features[:-6].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier to match the number of classes in your dataset\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        num_classes = model_args['num_classes'] \n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "            )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def init_dependencies(self):\n",
    "        return [\n",
    "            \"import pandas as pd\",\n",
    "            \"from PIL import Image\",\n",
    "            \"from torchvision import datasets, transforms, models\",\n",
    "            \"from torch.utils.data import Dataset\",\n",
    "            \"import torch.optim as optim\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    def init_optimizer(self, optimizer_args):        \n",
    "        return optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])\n",
    "\n",
    "    # training data\n",
    "    class CustomDatasetFedBioMed(Dataset):\n",
    "        def __init__(self, dataset_path: str, transform=None):\n",
    "            self.dataframe = pd.read_csv(dataset_path, index_col=0)\n",
    "            \n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),  \n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(30),\n",
    "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "           ])\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.dataframe)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            \n",
    "            img_path = self.dataframe.iloc[idx, 0]  # the file path is in the first column\n",
    "            print(img_path)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "            label = self.dataframe.iloc[idx, 1]  # the class label is in the second column\n",
    "    \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "    \n",
    "            label = torch.tensor(label, dtype=torch.long)  # if it returns a tuple instead of a tensor\n",
    "    \n",
    "            return image, label \n",
    "\n",
    "    def training_data(self):\n",
    "        dataset = self.CustomDatasetFedBioMed(self.dataset_path)\n",
    "        return DataManager(dataset)\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        loss   = loss_func(output, target)\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7db972-86fb-4c43-a55b-7b92afafc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 32, }, \n",
    "    'optimizer_args': {'lr': 1e-4}, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'num_classes': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63702f14-45a4-43d7-b2b5-8545f036840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags =  ['cancer']\n",
    "rounds = 1\n",
    "\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 training_plan_class=MyTrainingPlan3,\n",
    "                 model_args=model_args,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage())\n",
    "\n",
    "\n",
    "from fedbiomed.common.metrics import MetricTypes\n",
    "exp.set_test_ratio(.1)\n",
    "exp.set_test_on_local_updates(True)\n",
    "exp.set_test_metric(MetricTypes.ACCURACY)\n",
    "\n",
    "exp.set_tensorboard(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b0102-4083-4380-aead-fbced9448b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "tensorboard_dir = environ['TENSORBOARD_RESULTS_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c368be-84c5-4324-ac2d-b3ef01c43592",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcea3c7-5a65-4b9e-8a28-3cfb17846359",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"$tensorboard_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a96a5d-614c-49fc-b9b4-8199ed138be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a82add-678b-48cb-a81f-29958203cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./training_planxxxxx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
