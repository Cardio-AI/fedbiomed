{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed-BioMed Researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates using a convolutional model in PyTorch for recognition of smiling faces, with a CelebA dataset split over 2 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the node up\n",
    "\n",
    "Install the CelebA dataset with the help of the [`README.md`](data/Celeba/README.md), inside the [`notebooks/data` folder](./data/Celeba).\n",
    "The script create 3 nodes with each their data. The dataset of the node 3 is used in this notebook as a testing set.  \n",
    "Therefore its not necessary to create a node and run the node 3  \n",
    "\n",
    "Before using script make sure the correct environment is setup, for the node environment, run : `source ./scripts/fedbiomed_environment node`  \n",
    "For the sake of testing the resulting model, this file uses the data from node 1 and 2 for training and the data from node 3 to test.\n",
    "You can create multiple nodes by adding a config parameter to the command controlling nodes, for example :  \n",
    "creating 2 nodes for training :  \n",
    " - `./scripts/fedbiomed_run node config node1.ini start`\n",
    " - `./scripts/fedbiomed_run node config node2.ini start`  \n",
    " \n",
    "adding data for each node :  \n",
    " - `./scripts/fedbiomed_run node config node1.ini add`\n",
    " - `./scripts/fedbiomed_run node config node2.ini add`\n",
    "\n",
    "It is necessary to previously configure at least 1 node:\n",
    "1. `./scripts/fedbiomed_run node config (ini file) add`\n",
    "  * Select option 4 (images) to add an image dataset to the node\n",
    "  * Add a name (eg: 'celeba') and the tag for the dataset (tag should contain `#celeba` as it is the tag used for this training) and finaly add the description\n",
    "  * Pick a data folder from the 3 generated inside `data/Celeba/celeba_preprocessed` (eg: `data_node_1`)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node config (ini file) list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node config (ini file) start`. Wait until you get `Starting task manager`. it means you are online.\n",
    "\n",
    "For the sake of testing the resulting model, only nodes 1 and 2 were started during training, datas from node 3 is used to test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment to train a model on the data found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a TorchTrainingPlan Net class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from fedbiomed.common.data import DataManager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "class CelebaTrainingPlan(TorchTrainingPlan):\n",
    "         \n",
    "    # Defines model \n",
    "    def init_model(self):\n",
    "        model = self.Net()\n",
    "        return model \n",
    "    \n",
    "    # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torch.utils.data import Dataset\",\n",
    "                \"from torchvision import transforms\",\n",
    "                \"import pandas as pd\",\n",
    "                \"from PIL import Image\",\n",
    "                \"import os\",\n",
    "                \"import numpy as np\"]\n",
    "        return deps\n",
    "\n",
    "    # Torch modules class\n",
    "    class Net(nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            #convolution layers\n",
    "            self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
    "            self.conv3 = nn.Conv2d(32, 32, 3, 1)\n",
    "            self.conv4 = nn.Conv2d(32, 32, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            # classifier\n",
    "            self.fc1 = nn.Linear(3168, 128)\n",
    "            self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            x = self.conv2(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            x = self.conv3(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            x = self.conv4(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            output = F.log_softmax(x, dim=1)\n",
    "            return output\n",
    "\n",
    "\n",
    "    class CelebaDataset(Dataset):\n",
    "        \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n",
    "        \n",
    "        # we dont load the full data of the images, we retrieve the image with the get item. \n",
    "        # in our case, each image is 218*178 * 3colors. there is 67533 images. this take at leas 7G of ram\n",
    "        # loading images when needed takes more time during training but it wont impact the ram usage as much as loading everything\n",
    "        def __init__(self, txt_path, img_dir, transform=None):\n",
    "            df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)\n",
    "            self.img_dir = img_dir\n",
    "            self.txt_path = txt_path\n",
    "            self.img_names = df.index.values\n",
    "            self.y = df['Smiling'].values\n",
    "            self.transform = transform\n",
    "            print(\"celeba dataset finished\")\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img = np.asarray(Image.open(os.path.join(self.img_dir,\n",
    "                                        self.img_names[index])))\n",
    "            img = transforms.ToTensor()(img)\n",
    "            label = self.y[index]\n",
    "            return img, label\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.y.shape[0]\n",
    "    \n",
    "    # The training_data creates the Dataloader to be used for training in the \n",
    "    # general class Torchnn of fedbiomed\n",
    "    def training_data(self):\n",
    "        dataset = self.CelebaDataset(self.dataset_path + \"/target.csv\", self.dataset_path + \"/data/\")\n",
    "        train_kwargs = {'shuffle': True}\n",
    "        return DataManager(dataset, **train_kwargs)\n",
    "    \n",
    "    # This function must return the loss to backward it \n",
    "    def training_step(self, data, target):\n",
    "        \n",
    "        output = self.model().forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 32, }, \n",
    "    'optimizer_args': {\n",
    "        'lr': 1e-3\n",
    "    }, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the federated model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an experiment\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 11:42:24,529 fedbiomed INFO - Searching dataset with data tags: ['#celeba'] for all nodes\n",
      "2023-09-18 11:42:34,555 fedbiomed INFO - Node selected for training -> node_22a09353-4d82-49b1-a6b2-ec27849f7d58\n",
      "Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "2023-09-18 11:42:34,583 fedbiomed DEBUG - using native torch optimizer\n",
      "2023-09-18 11:42:34,587 fedbiomed DEBUG - Model file has been saved: /Users/edemairy/Developpement/fedbiomed/var/experiments/Experiment_0083/my_model_a05d0554-90ff-4ed8-8952-97082c991b8d.py\n",
      "2023-09-18 11:42:34,669 fedbiomed DEBUG - HTTP POST request of file /Users/edemairy/Developpement/fedbiomed/var/experiments/Experiment_0083/my_model_a05d0554-90ff-4ed8-8952-97082c991b8d.py successful, with status code 201\n",
      "2023-09-18 11:42:34,827 fedbiomed DEBUG - HTTP POST request of file /Users/edemairy/Developpement/fedbiomed/var/experiments/Experiment_0083/aggregated_params_200addf4-db1b-4afc-989a-cf4dc8088adc.mpk successful, with status code 201\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#celeba']\n",
    "rounds = 3\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 training_plan_class=CelebaTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 11:42:34,853 fedbiomed INFO - Sampled nodes in round 0 ['node_22a09353-4d82-49b1-a6b2-ec27849f7d58']\n",
      "2023-09-18 11:42:34,871 fedbiomed INFO - \u001B[1mSending request\u001B[0m \n",
      "\t\t\t\t\t\u001B[1m To\u001B[0m: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t\u001B[1m Request: \u001B[0m: Perform training with the arguments: {'researcher_id': 'researcher_ad35dfc5-da2c-4bf9-bb84-e61b3b9434b3', 'job_id': '25334363-0bad-486a-aca8-1b1a4a0acefc', 'training_args': {'loader_args': {'batch_size': 32}, 'optimizer_args': {'lr': 0.001}, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100, 'num_updates': None, 'test_ratio': 0.0, 'test_on_local_updates': False, 'test_on_global_updates': False, 'test_metric': None, 'test_metric_args': {}, 'log_interval': 10, 'fedprox_mu': None, 'use_gpu': False, 'dp_args': None, 'share_persistent_buffers': True, 'random_seed': None}, 'training': True, 'model_args': {}, 'round': 0, 'secagg_servkey_id': None, 'secagg_biprime_id': None, 'secagg_random': None, 'secagg_clipping_range': None, 'command': 'train', 'aux_var_urls': None, 'training_plan_url': 'http://localhost:8844/media/uploads/2023/09/18/my_model_a05d0554-90ff-4ed8-8952-97082c991b8d.py', 'params_url': 'http://localhost:8844/media/uploads/2023/09/18/aggregated_params_200addf4-db1b-4afc-989a-cf4dc8088adc.mpk', 'training_plan_class': 'CelebaTrainingPlan', 'dataset_id': 'dataset_0a9da800-3a15-4c04-80e6-6bef64570e3c'} \n",
      " -----------------------------------------------------------------\n",
      "2023-09-18 11:42:34,873 fedbiomed DEBUG - researcher_ad35dfc5-da2c-4bf9-bb84-e61b3b9434b3\n",
      "2023-09-18 11:42:35,028 fedbiomed INFO - \u001B[1mWARNING\u001B[0m\n",
      "\t\t\t\t\t\u001B[1m NODE\u001B[0m node_22a09353-4d82-49b1-a6b2-ec27849f7d58\n",
      "\t\t\t\t\t\u001B[1m MESSAGE:\u001B[0m There is no validation activated for the round. Please set flag for `test_on_global_updates`, `test_on_local_updates`, or both. Splitting dataset for validation will be ignored\u001B[0m\n",
      "-----------------------------------------------------------------\n",
      "2023-09-18 11:42:36,340 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 1/100 (1%) | Samples: 32/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.684615\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:42:45,019 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 10/100 (10%) | Samples: 320/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.697778\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:43:19,150 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 20/100 (20%) | Samples: 640/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.700031\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:43:32,020 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 30/100 (30%) | Samples: 960/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.692787\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:43:43,965 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 40/100 (40%) | Samples: 1280/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.680594\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:43:56,837 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 50/100 (50%) | Samples: 1600/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.663382\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:44:08,704 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 60/100 (60%) | Samples: 1920/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.692889\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:44:20,482 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 70/100 (70%) | Samples: 2240/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.699375\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:44:32,304 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 80/100 (80%) | Samples: 2560/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.725884\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:44:45,521 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 90/100 (90%) | Samples: 2880/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.672462\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:44:57,900 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 1 Epoch: 1 | Iteration: 100/100 (100%) | Samples: 3200/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.670723\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:44:58,115 fedbiomed INFO - \u001B[1mINFO\u001B[0m\n",
      "\t\t\t\t\t\u001B[1m NODE\u001B[0m node_22a09353-4d82-49b1-a6b2-ec27849f7d58\n",
      "\t\t\t\t\t\u001B[1m MESSAGE:\u001B[0m results uploaded successfully \u001B[0m\n",
      "-----------------------------------------------------------------\n",
      "2023-09-18 11:45:04,221 fedbiomed INFO - Downloading model params after training on node_22a09353-4d82-49b1-a6b2-ec27849f7d58 - from http://localhost:8844/media/uploads/2023/09/18/node_params_eba75851-bdc1-4550-bb7c-1b89450470be.mpk\n",
      "2023-09-18 11:45:04,306 fedbiomed DEBUG - download of file node_params_5b083f9f-eedc-438f-aa45-4b1e6d90ec25.mpk successful, with status code 200\n",
      "2023-09-18 11:45:04,337 fedbiomed INFO - Nodes that successfully reply in round 0 ['node_22a09353-4d82-49b1-a6b2-ec27849f7d58']\n",
      "2023-09-18 11:45:04,507 fedbiomed DEBUG - HTTP POST request of file /Users/edemairy/Developpement/fedbiomed/var/experiments/Experiment_0083/aggregated_params_e8e8f46f-f23b-402a-b128-7ab5f847f9b0.mpk successful, with status code 201\n",
      "2023-09-18 11:45:04,509 fedbiomed INFO - Saved aggregated params for round 0 in /Users/edemairy/Developpement/fedbiomed/var/experiments/Experiment_0083/aggregated_params_e8e8f46f-f23b-402a-b128-7ab5f847f9b0.mpk\n",
      "2023-09-18 11:45:04,510 fedbiomed INFO - Sampled nodes in round 1 ['node_22a09353-4d82-49b1-a6b2-ec27849f7d58']\n",
      "2023-09-18 11:45:04,512 fedbiomed INFO - \u001B[1mSending request\u001B[0m \n",
      "\t\t\t\t\t\u001B[1m To\u001B[0m: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t\u001B[1m Request: \u001B[0m: Perform training with the arguments: {'researcher_id': 'researcher_ad35dfc5-da2c-4bf9-bb84-e61b3b9434b3', 'job_id': '25334363-0bad-486a-aca8-1b1a4a0acefc', 'training_args': {'loader_args': {'batch_size': 32}, 'optimizer_args': {'lr': 0.001}, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100, 'num_updates': None, 'test_ratio': 0.0, 'test_on_local_updates': False, 'test_on_global_updates': False, 'test_metric': None, 'test_metric_args': {}, 'log_interval': 10, 'fedprox_mu': None, 'use_gpu': False, 'dp_args': None, 'share_persistent_buffers': True, 'random_seed': None}, 'training': True, 'model_args': {}, 'round': 1, 'secagg_servkey_id': None, 'secagg_biprime_id': None, 'secagg_random': None, 'secagg_clipping_range': None, 'command': 'train', 'aux_var_urls': None, 'training_plan_url': 'http://localhost:8844/media/uploads/2023/09/18/my_model_a05d0554-90ff-4ed8-8952-97082c991b8d.py', 'params_url': 'http://localhost:8844/media/uploads/2023/09/18/aggregated_params_e8e8f46f-f23b-402a-b128-7ab5f847f9b0.mpk', 'training_plan_class': 'CelebaTrainingPlan', 'dataset_id': 'dataset_0a9da800-3a15-4c04-80e6-6bef64570e3c'} \n",
      " -----------------------------------------------------------------\n",
      "2023-09-18 11:45:04,515 fedbiomed DEBUG - researcher_ad35dfc5-da2c-4bf9-bb84-e61b3b9434b3\n",
      "2023-09-18 11:45:04,677 fedbiomed INFO - \u001B[1mWARNING\u001B[0m\n",
      "\t\t\t\t\t\u001B[1m NODE\u001B[0m node_22a09353-4d82-49b1-a6b2-ec27849f7d58\n",
      "\t\t\t\t\t\u001B[1m MESSAGE:\u001B[0m There is no validation activated for the round. Please set flag for `test_on_global_updates`, `test_on_local_updates`, or both. Splitting dataset for validation will be ignored\u001B[0m\n",
      "-----------------------------------------------------------------\n",
      "2023-09-18 11:45:06,083 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 1/100 (1%) | Samples: 32/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.679334\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:45:16,717 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 10/100 (10%) | Samples: 320/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.669346\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:45:28,437 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 20/100 (20%) | Samples: 640/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.693436\u001B[0m \n",
      "\t\t\t\t\t ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 11:45:40,093 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 30/100 (30%) | Samples: 960/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.618258\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:45:52,005 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 40/100 (40%) | Samples: 1280/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.661937\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:46:03,810 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 50/100 (50%) | Samples: 1600/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.645756\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:46:15,482 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 60/100 (60%) | Samples: 1920/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.533509\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:46:27,407 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 70/100 (70%) | Samples: 2240/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.597947\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:46:39,199 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 80/100 (80%) | Samples: 2560/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.627661\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:46:51,006 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 90/100 (90%) | Samples: 2880/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.739260\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:47:02,714 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 2 Epoch: 1 | Iteration: 100/100 (100%) | Samples: 3200/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.632026\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:47:02,900 fedbiomed INFO - \u001B[1mINFO\u001B[0m\n",
      "\t\t\t\t\t\u001B[1m NODE\u001B[0m node_22a09353-4d82-49b1-a6b2-ec27849f7d58\n",
      "\t\t\t\t\t\u001B[1m MESSAGE:\u001B[0m results uploaded successfully \u001B[0m\n",
      "-----------------------------------------------------------------\n",
      "2023-09-18 11:47:09,719 fedbiomed INFO - Downloading model params after training on node_22a09353-4d82-49b1-a6b2-ec27849f7d58 - from http://localhost:8844/media/uploads/2023/09/18/node_params_bcfb8137-4885-4345-8e31-f0f71e7d7767.mpk\n",
      "2023-09-18 11:47:09,782 fedbiomed DEBUG - download of file node_params_632648af-0641-4858-9cc9-fe534703e584.mpk successful, with status code 200\n",
      "2023-09-18 11:47:09,791 fedbiomed INFO - Nodes that successfully reply in round 1 ['node_22a09353-4d82-49b1-a6b2-ec27849f7d58']\n",
      "2023-09-18 11:47:10,019 fedbiomed DEBUG - HTTP POST request of file /Users/edemairy/Developpement/fedbiomed/var/experiments/Experiment_0083/aggregated_params_1ddfc8d6-988f-4578-b061-b79bcfc32cb1.mpk successful, with status code 201\n",
      "2023-09-18 11:47:10,021 fedbiomed INFO - Saved aggregated params for round 1 in /Users/edemairy/Developpement/fedbiomed/var/experiments/Experiment_0083/aggregated_params_1ddfc8d6-988f-4578-b061-b79bcfc32cb1.mpk\n",
      "2023-09-18 11:47:10,023 fedbiomed INFO - Sampled nodes in round 2 ['node_22a09353-4d82-49b1-a6b2-ec27849f7d58']\n",
      "2023-09-18 11:47:10,024 fedbiomed INFO - \u001B[1mSending request\u001B[0m \n",
      "\t\t\t\t\t\u001B[1m To\u001B[0m: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t\u001B[1m Request: \u001B[0m: Perform training with the arguments: {'researcher_id': 'researcher_ad35dfc5-da2c-4bf9-bb84-e61b3b9434b3', 'job_id': '25334363-0bad-486a-aca8-1b1a4a0acefc', 'training_args': {'loader_args': {'batch_size': 32}, 'optimizer_args': {'lr': 0.001}, 'epochs': 1, 'dry_run': False, 'batch_maxnum': 100, 'num_updates': None, 'test_ratio': 0.0, 'test_on_local_updates': False, 'test_on_global_updates': False, 'test_metric': None, 'test_metric_args': {}, 'log_interval': 10, 'fedprox_mu': None, 'use_gpu': False, 'dp_args': None, 'share_persistent_buffers': True, 'random_seed': None}, 'training': True, 'model_args': {}, 'round': 2, 'secagg_servkey_id': None, 'secagg_biprime_id': None, 'secagg_random': None, 'secagg_clipping_range': None, 'command': 'train', 'aux_var_urls': None, 'training_plan_url': 'http://localhost:8844/media/uploads/2023/09/18/my_model_a05d0554-90ff-4ed8-8952-97082c991b8d.py', 'params_url': 'http://localhost:8844/media/uploads/2023/09/18/aggregated_params_1ddfc8d6-988f-4578-b061-b79bcfc32cb1.mpk', 'training_plan_class': 'CelebaTrainingPlan', 'dataset_id': 'dataset_0a9da800-3a15-4c04-80e6-6bef64570e3c'} \n",
      " -----------------------------------------------------------------\n",
      "2023-09-18 11:47:10,030 fedbiomed DEBUG - researcher_ad35dfc5-da2c-4bf9-bb84-e61b3b9434b3\n",
      "2023-09-18 11:47:10,151 fedbiomed INFO - \u001B[1mWARNING\u001B[0m\n",
      "\t\t\t\t\t\u001B[1m NODE\u001B[0m node_22a09353-4d82-49b1-a6b2-ec27849f7d58\n",
      "\t\t\t\t\t\u001B[1m MESSAGE:\u001B[0m There is no validation activated for the round. Please set flag for `test_on_global_updates`, `test_on_local_updates`, or both. Splitting dataset for validation will be ignored\u001B[0m\n",
      "-----------------------------------------------------------------\n",
      "2023-09-18 11:47:11,377 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 1/100 (1%) | Samples: 32/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.637949\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:47:21,864 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 10/100 (10%) | Samples: 320/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.614381\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:47:34,543 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 20/100 (20%) | Samples: 640/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.512903\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:47:46,730 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 30/100 (30%) | Samples: 960/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.631536\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:47:58,561 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 40/100 (40%) | Samples: 1280/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.524485\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:48:10,550 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 50/100 (50%) | Samples: 1600/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.427811\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:48:22,212 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 60/100 (60%) | Samples: 1920/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.480740\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:48:34,003 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 70/100 (70%) | Samples: 2240/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.420297\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:48:45,789 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 80/100 (80%) | Samples: 2560/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.375515\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:48:57,768 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 90/100 (90%) | Samples: 2880/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.474186\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:49:09,520 fedbiomed INFO - \u001B[1mTRAINING\u001B[0m \n",
      "\t\t\t\t\t NODE_ID: node_22a09353-4d82-49b1-a6b2-ec27849f7d58 \n",
      "\t\t\t\t\t Round 3 Epoch: 1 | Iteration: 100/100 (100%) | Samples: 3200/3200\n",
      " \t\t\t\t\t Loss: \u001B[1m0.397970\u001B[0m \n",
      "\t\t\t\t\t ---------\n",
      "2023-09-18 11:49:09,734 fedbiomed INFO - \u001B[1mINFO\u001B[0m\n",
      "\t\t\t\t\t\u001B[1m NODE\u001B[0m node_22a09353-4d82-49b1-a6b2-ec27849f7d58\n",
      "\t\t\t\t\t\u001B[1m MESSAGE:\u001B[0m results uploaded successfully \u001B[0m\n",
      "-----------------------------------------------------------------\n",
      "2023-09-18 11:49:15,240 fedbiomed INFO - Downloading model params after training on node_22a09353-4d82-49b1-a6b2-ec27849f7d58 - from http://localhost:8844/media/uploads/2023/09/18/node_params_ce3fa78d-eb56-44ef-b907-cd2d6a639834.mpk\n",
      "2023-09-18 11:49:15,308 fedbiomed DEBUG - download of file node_params_264bab58-64f6-423b-8409-d18e0a0146c9.mpk successful, with status code 200\n",
      "2023-09-18 11:49:15,316 fedbiomed INFO - Nodes that successfully reply in round 2 ['node_22a09353-4d82-49b1-a6b2-ec27849f7d58']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 11:49:15,530 fedbiomed DEBUG - HTTP POST request of file /Users/edemairy/Developpement/fedbiomed/var/experiments/Experiment_0083/aggregated_params_ed6a33f4-c160-45ff-8d4b-9925352cd606.mpk successful, with status code 201\n",
      "2023-09-18 11:49:15,531 fedbiomed INFO - Saved aggregated params for round 2 in /Users/edemairy/Developpement/fedbiomed/var/experiments/Experiment_0083/aggregated_params_ed6a33f4-c160-45ff-8d4b-9925352cd606.mpk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the federated model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed_model = exp.training_plan().model()\n",
    "fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=3168, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a little testing routine to extract the accuracy metrics on the testing dataset\n",
    "## Important\n",
    "This is done to test the model because it can be accessed in a developpement environment  \n",
    "In production, the data wont be accessible on the nodes, we will need a test dataset on the server or accessible from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def testing_Accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    device = \"cpu\"\n",
    "\n",
    "    correct = 0\n",
    "    \n",
    "    loader_size = len(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            #only uses 10% of the dataset, results are similar but faster\n",
    "            if idx >= loader_size / 10:\n",
    "                pass\n",
    "                break\n",
    "\n",
    "    \n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100* correct/(data_loader.batch_size * idx)\n",
    "\n",
    "    return(test_loss, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset is the data from the third node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celeba dataset finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset_path = \"./data/Celeba/celeba_preprocessed/data_node_3\"\n",
    "\n",
    "class CelebaDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n",
    "\n",
    "    def __init__(self, txt_path, img_dir, transform=None):\n",
    "        df = pd.read_csv(txt_path, sep=\"\\t\", index_col=0)\n",
    "        self.img_dir = img_dir\n",
    "        self.txt_path = txt_path\n",
    "        self.img_names = df.index.values\n",
    "        self.y = df['Smiling'].values\n",
    "        self.transform = transform\n",
    "        print(\"celeba dataset finished\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = np.asarray(Image.open(os.path.join(self.img_dir,\n",
    "                                        self.img_names[index])))\n",
    "        img = transforms.ToTensor()(img)\n",
    "        label = self.y[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "\n",
    "dataset = CelebaDataset(test_dataset_path + \"/target.csv\", test_dataset_path + \"/data/\")\n",
    "train_kwargs = { 'shuffle': True}\n",
    "data_loader = DataLoader(dataset, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the testing dataset and computing accuracy metrics for local and federated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_federated = testing_Accuracy(fed_model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.96624222682854"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_federated[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e89561e616b27d96972869796636c18d9df047835da4fde0d47d1d63cf19e486"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
