{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05008326",
   "metadata": {},
   "source": [
    "# Privacy Preserving Training and Inference Machine Learning over Medical Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4d762-33c1-4ed1-85b2-a71dd6658897",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this tutorial, we will demonstrate the process of privacy-preserving training using Fed-BioMed, which leverages federated learning with secure aggregation. Subsequently, we will deploy the final model obtained through federated learning in a privacy-preserving manner using the [Concrete-ML](https://github.com/zama-ai/concrete-ml) library. This approach allows us to achieve privacy-preserving inference through a software-as-a-service (SaaS) model.\n",
    "\n",
    "The selected dataset originates from a medical task assigned by [Flamby](https://github.com/owkin/FLamby), specifically the FedHeart Disease dataset. For more detailed information about the dataset, please refer to the provided [link](https://github.com/owkin/FLamby/blob/main/flamby/datasets/fed_heart_disease/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73897222",
   "metadata": {},
   "source": [
    "## Install Concrete-ML\n",
    "\n",
    "This tutorial assumes you have Concrete-ML installed in your Fed-BioMed researcher environment. \n",
    "\n",
    "If needed, you may install it with:\n",
    "1. `source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher`\n",
    "2. `pip install concrete-ml`\n",
    "\n",
    "### Note for MacOS users with ARM chips\n",
    "\n",
    "If you have a recent Mac machine with Apple Silicon (ARM chips), then you may experience kernel failure in Section 3.2 of this notebook. \n",
    "\n",
    "To overcome this issue, you need to rebuild your conda `fedbiomed-researcher` environment with a native python executable, before installing Concrete-ML. This process may take some time. \n",
    "\n",
    "1. `export CONDA_SUBDIR=osx-arm64`\n",
    "2. `${FEDBIOMED_DIR}/scripts/configure_conda -c researcher`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506aa36",
   "metadata": {},
   "source": [
    "##  Flamby configuration - Download FedHeart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfe0c6",
   "metadata": {},
   "source": [
    "You need to download the FLamby dataset that we will use. For licensing reasons, these are not including directly in the FLamby installation.\n",
    "\n",
    "To download the fed_heart dataset in `${FEDBIOMED_DIR}/data` (where `${FEDBIOMED_DIR}` is the base directory of Fed-BioMed): \n",
    "\n",
    "1. `source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment node`<br>\n",
    "2. `pip install wget`<br>\n",
    "3. `python ${FEDBIOMED_DIR}/docs/tutorials/concrete-ml/download.py --output-folder ${FEDBIOMED_DIR}/data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8eda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from flamby.datasets.fed_heart_disease import FedHeartDisease\n",
    "from fedbiomed.common.data import FlambyDataset, DataManager\n",
    "from concrete.ml.torch.compile import compile_torch_model\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torch.nn as nn\n",
    "from fedbiomed.researcher.environ import environ\n",
    "tensorboard_dir = environ['TENSORBOARD_RESULTS_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEDBIOMED_DIR = os.getenv('FEDBIOMED_DIR')\n",
    "DATASET_TEST_PATH = f\"{FEDBIOMED_DIR}/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265443e",
   "metadata": {},
   "source": [
    "## 1. Fed-BioMed\n",
    "\n",
    "Configuring the Fed-BioMed training plan involves specifying the machine learning model, defining the loss function, and identifying the necessary dependencies. This ensures a clear and well-defined setup for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4991919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7120f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedHeartTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    class Baseline(nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(13, 16)\n",
    "            self.fc2 = nn.Linear(16, 2)\n",
    "            self.act = nn.LeakyReLU()\n",
    "        def forward(self, x):\n",
    "            x = self.act(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "        \n",
    "    class BaselineLoss(_Loss):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, prediction: torch.Tensor, target: torch.Tensor):\n",
    "            target = torch.squeeze(target, dim=1).type(torch.long)\n",
    "            return self.ce(prediction, target)\n",
    "    \n",
    "    def init_model(self, model_args):\n",
    "        return self.Baseline()\n",
    "\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return torch.optim.AdamW(self.model().parameters(), lr=optimizer_args[\"lr\"])\n",
    "\n",
    "    def init_dependencies(self):\n",
    "        return [\"from flamby.datasets.fed_heart_disease import FedHeartDisease\",\n",
    "                \"from torch.nn.modules.loss import _Loss\",\n",
    "                \"from fedbiomed.common.data import FlambyDataset, DataManager\"]\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        logits = self.model().forward(data)\n",
    "        return self.BaselineLoss().forward(logits, target)\n",
    "\n",
    "    def training_data(self, batch_size=2):\n",
    "        dataset = FlambyDataset()\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "num_updates=10\n",
    "def get_nb_max_rounds(num_updates, batch_size, num_epochs_pooled):\n",
    "    return (486 // 4 // batch_size) * num_epochs_pooled // num_updates\n",
    "num_rounds = get_nb_max_rounds(num_updates=num_updates, batch_size=batch_size, num_epochs_pooled=50)\n",
    "num_rounds\n",
    "print(num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c474482",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'optimizer_args': {\n",
    "        'lr': 5e-4,\n",
    "    },\n",
    "    'loader_args': {\n",
    "        'batch_size': batch_size,\n",
    "    },\n",
    "    'num_updates': num_updates,\n",
    "    'dry_run': False,\n",
    "    'log_interval': 2,\n",
    "    'test_ratio' : 0.0,\n",
    "    'test_on_global_updates': False,\n",
    "    'test_on_local_updates': False,\n",
    "    'random_seed':42,\n",
    "}\n",
    "\n",
    "model_args = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b8b65",
   "metadata": {},
   "source": [
    "## 2. Federated Learning Training with SecAgg\n",
    "\n",
    "**Nodes Configuration:**\n",
    "For each node in the range `i in [0...3]`:\n",
    "\n",
    "1. Open a new terminal.\n",
    "2. Run the command: `./scripts/fedbiomed_run node --config config-n{i}.ini dataset add`\n",
    "3. Select `6) flamby`.\n",
    "4. Enter the dataset name: `flamby` (optional).\n",
    "5. Set tags to: `heart` (important).\n",
    "6. Description: Enter `none` (optional).\n",
    "7. Select `1) fed_heart_disease`.\n",
    "8. Specify a center ID between 0 and 3: `{i}`.\n",
    "9. Description: Enter `none` (optional).\n",
    "10. Run the command: `./scripts/fedbiomed_run node --config config-n{i}.ini start`.\n",
    "\n",
    "### Configuring Secure Aggregation\n",
    "\n",
    "If you haven't done so already, you need to configure SecAgg in Fed-BioMed. \n",
    "A quick reminder of the basic commands is given below, but please refer to our documentation for a full explanation.\n",
    "\n",
    "1. `${FEDBIOMED_DIR}/scripts/fedbiomed_configure_secagg researcher`\n",
    "2. make sure the researcher and nodes' config files have been created (e.g. step 2 above)\n",
    "3. `${FEDBIOMED_DIR}/scripts/fedbiomed_run certificate-dev-setup`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['heart']\n",
    "\n",
    "exp_sec_agg = Experiment(tags=tags,\n",
    "                 training_plan_class=FedHeartTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 model_args=model_args,\n",
    "                 round_limit=num_rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 secagg=True,\n",
    "                 tensorboard=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd042d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir \"$tensorboard_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f383ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_sec_agg.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cb686",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_sec_agg_model = exp_sec_agg.training_plan().model()\n",
    "fed_sec_agg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FedHeartDisease(center=0,pooled=True, train=False, data_path=DATASET_TEST_PATH)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c6bb1f",
   "metadata": {},
   "source": [
    "## 3.1. Inference with torch (over a plaintext model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43077e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_torch(net, test_loader):\n",
    "    \"\"\"Test the network: measure accuracy on the test set.\"\"\"\n",
    "\n",
    "    # Freeze normalization layers\n",
    "    net.eval()\n",
    "\n",
    "    all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "    all_targets = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "\n",
    "    # Iterate over the batches\n",
    "    idx = 0\n",
    "    for data, target in test_loader:\n",
    "        # Accumulate the ground truth labels\n",
    "        endidx = idx + target.shape[0]\n",
    "        all_targets[idx:endidx] = target.numpy()\n",
    "\n",
    "        # Run forward and get the predicted class id\n",
    "        logits = torch.sigmoid(net(data))\n",
    "        output = logits.argmax(1).detach().numpy()\n",
    "        all_y_pred[idx:endidx] = output\n",
    "\n",
    "        idx += target.shape[0]\n",
    "\n",
    "    # Print out the accuracy as a percentage\n",
    "    n_correct = np.sum(all_targets == all_y_pred)\n",
    "    print(\n",
    "        f\"Test accuracy over plaintext model: \"\n",
    "        f\"{n_correct / len(test_loader) * 100:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_torch(fed_sec_agg_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd3a4d",
   "metadata": {},
   "source": [
    "## 3.2. Inference with Concrete-ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_concrete(quantized_module, test_loader, use_sim):\n",
    "    \"\"\"Test a neural network that is quantized and compiled with Concrete ML.\"\"\"\n",
    "\n",
    "    # Casting the inputs into int64 is recommended\n",
    "    all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "    all_targets = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "\n",
    "    # Iterate over the test batches and accumulate predictions and ground truth labels in a vector\n",
    "    idx = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.numpy()\n",
    "        target = target.numpy()\n",
    "\n",
    "        fhe_mode = \"simulate\" if use_sim else \"execute\"\n",
    "\n",
    "        # Quantize the inputs and cast to appropriate data type\n",
    "        logits = torch.tensor(quantized_module.forward(data, fhe=fhe_mode), requires_grad=False)\n",
    "\n",
    "        endidx = idx + target.shape[0]\n",
    "\n",
    "        all_targets[idx:endidx] = target\n",
    "\n",
    "        # Get the predicted class id and accumulate the predictions\n",
    "        y_pred = torch.sigmoid(logits).argmax(1).numpy()\n",
    "        all_y_pred[idx:endidx] = y_pred\n",
    "\n",
    "        # Update the index\n",
    "        idx += target.shape[0]\n",
    "    n_correct = np.sum(all_targets == all_y_pred)\n",
    "    print(\n",
    "        f\"Test accuracy over encrypted model: \"\n",
    "        f\"{n_correct / len(test_loader) * 100:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033712c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concrete ml is using the traceback, \n",
    "# while fed-biomed for logging reasons fixs it to 3, to use concrete-ml we reset to the default value\n",
    "import sys\n",
    "sys.tracebacklimit = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bits = 6\n",
    "compile_set = np.random.randint(0, 10, (100, 13)).astype(float)\n",
    "q_module = compile_torch_model(fed_sec_agg_model, compile_set, rounding_threshold_bits=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_concrete(q_module, test_dataloader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefe7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
